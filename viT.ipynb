{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPdqfvQSd3eP",
        "outputId": "9e404bb7-d0cb-4fa6-8b8a-b8cab378576b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm  # For progress bars\n",
        "\n",
        "# Use GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2  # Number of patches\n",
        "\n",
        "        # Use Conv2d to split image into patches\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_channels=in_channels,       # Input channels (3 for RGB)\n",
        "            out_channels=embed_dim,        # Output dimension (D)\n",
        "            kernel_size=patch_size,        # Patch size (e.g., 16x16)\n",
        "            stride=patch_size,             # Non-overlapping patches\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: (B, C, H, W) → (Batch, Channels, Height, Width)\n",
        "        x = self.proj(x)  # (B, D, H/P, W/P) → (B, 768, 14, 14) if img_size=224, patch_size=16\n",
        "        x = x.flatten(2)  # Flatten patches into sequence → (B, D, N_patches)\n",
        "        x = x.transpose(1, 2)  # (B, N_patches, D) → Now it's like a sequence!\n",
        "        return x"
      ],
      "metadata": {
        "id": "poCRwGmb2s37"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),                 # Resize to 224x224\n",
        "    transforms.ToTensor(),                  # Convert to tensor\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 datasets\n",
        "train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create dataloaders (batches of 64 images)\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "CHVmV_SreELd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee3b766-7a1a-40af-fbf9-a39f424f7724"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:10<00:00, 15.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, n_patches, embed_dim):\n",
        "        super().__init__()\n",
        "        # Learnable parameter: (1, N+1, D) (+1 for [CLS] token)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, n_patches + 1, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pos_embed  # Add position info to patches"
      ],
      "metadata": {
        "id": "DCd4Z06Z2feM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim=768, n_heads=12):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = embed_dim // n_heads  # 768 / 12 = 64\n",
        "\n",
        "        # Linear layers to compute Q, K, V in one go\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)  # Final projection\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, D = x.shape\n",
        "        # Compute Q, K, V (each B, N, D) → split into heads\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # Each (B, n_heads, N, head_dim)\n",
        "\n",
        "        # Attention scores (B, n_heads, N, N)\n",
        "        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        # Weighted sum of values\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, D)\n",
        "        x = self.proj(x)  # Final linear layer\n",
        "        return x"
      ],
      "metadata": {
        "id": "U5B8Tm7n2fpL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim=768, n_heads=12, mlp_ratio=4.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = MultiHeadAttention(embed_dim, n_heads)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),  # Expand\n",
        "            nn.GELU(),                                         # Activation\n",
        "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim),  # Compress\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. LayerNorm → MSA → Residual\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        # 2. LayerNorm → MLP → Residual\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "wSv1ZS962f0X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, n_classes=1000,\n",
        "                 embed_dim=768, depth=12, n_heads=12):\n",
        "        super().__init__()\n",
        "        # 1. Split image into patches\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "\n",
        "        # 2. [CLS] token (for classification)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "\n",
        "        # 3. Positional embeddings\n",
        "        self.pos_embed = PositionalEncoding(self.patch_embed.n_patches, embed_dim)\n",
        "\n",
        "        # 4. Stack of Transformer blocks\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerBlock(embed_dim, n_heads)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        # 5. Final normalization & classification head\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]  # Batch size\n",
        "\n",
        "        # 1. Patch embedding (B, N, D)\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # 2. Add [CLS] token (B, 1, D)\n",
        "        cls_token = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat([cls_token, x], dim=1)  # (B, N+1, D)\n",
        "\n",
        "        # 3. Add positional embeddings\n",
        "        x = self.pos_embed(x)\n",
        "\n",
        "        # 4. Transformer blocks\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # 5. Take [CLS] token and classify\n",
        "        x = self.norm(x[:, 0])  # Only [CLS] token (B, D)\n",
        "        x = self.head(x)         # (B, n_classes)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-Np_m0m42U3B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    vit = VisionTransformer(img_size=224, patch_size=16, n_classes=10)\n",
        "    img = torch.randn(1, 3, 224, 224)  # Fake image\n",
        "    out = vit(img)\n",
        "    print(out.shape)  # (1, 10) → 10-class prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUIGilc422WC",
        "outputId": "d3925658-9a5b-441c-9dd5-86f4117e3969"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer(\n",
        "    img_size=224,       # Resized CIFAR-10 images\n",
        "    patch_size=16,       # Smaller patches for smaller images\n",
        "    in_channels=3,       # RGB\n",
        "    n_classes=10,        # CIFAR-10 has 10 classes\n",
        "    embed_dim=768,       # From original ViT\n",
        "    depth=6,             # Fewer layers for faster training (original: 12)\n",
        "    n_heads=12,          # As in original ViT\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "NP0xAnFKeEYK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Lower LR for stability"
      ],
      "metadata": {
        "id": "FzXV6bPheEq_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, criterion, optimizer, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for images, labels in progress_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update progress\n",
        "            running_loss += loss.item()\n",
        "            progress_bar.set_postfix(loss=running_loss / len(dataloader))\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "p27M_BcLeE6_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for 5 epochs\n",
        "train(model, train_loader, criterion, optimizer, epochs=1)"
      ],
      "metadata": {
        "id": "gzpLSdWSeFHU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04cbc5a9-164a-454a-eda4-4d01f0e38b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   5%|▍         | 37/782 [40:48<13:54:58, 67.25s/it, loss=0.128]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Testing\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "TCPM0q1UeFT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "test(model, test_loader)"
      ],
      "metadata": {
        "id": "asY-U4n7eFfp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}